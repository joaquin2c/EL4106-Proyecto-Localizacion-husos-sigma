{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Cargamos los datos a trabajar\n",
    "\n",
    "x_train_pre=np.load(\"./data/train_signals.npy\")\n",
    "y_train_pre=np.load(\"./data/train_marks.npy\")\n",
    "trainLen=len(x_train_pre)\n",
    "\n",
    "x_val_pre=np.load(\"./data/val_signals.npy\")\n",
    "y_val_pre=np.load(\"./data/val_marks.npy\")\n",
    "valLen=len(x_val_pre)\n",
    "\n",
    "x_test_pre=np.load(\"./data/test_signals.npy\")\n",
    "y_test_pre=np.load(\"./data/test_marks.npy\")\n",
    "testLen=len(x_test_pre)\n",
    "\n",
    "#Creamos una funcion que muestree a un ratio deseado\n",
    "\n",
    "def sampling(signals,samples,lenght,rate):\n",
    "    x_new=[]\n",
    "    for i in signals:\n",
    "        x=[]\n",
    "        for j in range(0,lenght,rate):\n",
    "            x.append(i[j])\n",
    "        x_new.append(x)\n",
    "    return np.array(x_new)\n",
    "\n",
    "#Obtenemos los nuevos resultados de nuestras señales a trabajar\n",
    "\n",
    "x_train=sampling(x_train_pre,trainLen,800,1)\n",
    "x_val=sampling(x_val_pre,valLen,800,1)\n",
    "x_test=sampling(x_test_pre,testLen,800,1)\n",
    "\n",
    "#Creamos una funcion que clasifique con 1 o 0 los valores de la señal a trabajar\n",
    "\n",
    "def oneHot(marks,lenght,rate):\n",
    "    y_new=[]\n",
    "    for i in marks:\n",
    "        y=[]\n",
    "        for j in range(0,lenght,rate):\n",
    "            if(i[0]<=j & j<=i[1]):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        y_new.append(y)\n",
    "    return np.array(y_new)\n",
    "\n",
    "#Obtenemos los nuevos resultados de nuestras marcas a trabajar\n",
    "\n",
    "y_train=oneHot(y_train_pre,800,1)\n",
    "y_val=oneHot(y_val_pre,800,1)\n",
    "y_test=oneHot(y_test_pre,800,1)\n",
    "\n",
    "x_train2=np.expand_dims(x_train,2)\n",
    "y_train2=np.expand_dims(y_train,2)\n",
    "\n",
    "x_val2=np.expand_dims(x_val,2)\n",
    "y_val2=np.expand_dims(y_val,2)\n",
    "\n",
    "x_test2=np.expand_dims(x_test,2)\n",
    "y_test2=np.expand_dims(y_test,2)\n",
    "\n",
    "BATCH_SIZE=20\n",
    "repeat=20\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train2, y_train)).shuffle(buffer_size=1024).batch(BATCH_SIZE).repeat(repeat)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test2, y_test)).shuffle(buffer_size=1024).batch(BATCH_SIZE).repeat(repeat)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val2, y_val)).shuffle(buffer_size=1024).batch(BATCH_SIZE).repeat(repeat)\n",
    "\n",
    "timexx=np.arange(0,800,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "\n",
    "    # Codificacion 'one hot' para las etiquetas de clase\n",
    "    n_classes = logits.shape[1]\n",
    "    one_hot_labels = tf.one_hot(labels, n_classes)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits,\n",
    "                    labels=one_hot_labels),\n",
    "                name='xentropy')\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        correct_predictions = tf.equal(labels, predictions)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes,\n",
    "        input_shape,\n",
    "        \n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        dropout_rate=0.5,\n",
    "        early_stopping=None,\n",
    "        logdir='logs'):    \n",
    "       \n",
    "        super().__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_shape_tuple = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.logdir = logdir\n",
    "        self.layers_list = self._init_layers(layer_sizes)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        \n",
    "    def _init_layers(self, layer_sizes):\n",
    "        \n",
    "        n_layers = len(layer_sizes)\n",
    "        layers_list = []\n",
    "        \n",
    "        layers_list.append(tf.keras.layers.InputLayer((self.input_shape_tuple)))\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layers_list.append(\n",
    "                tf.keras.layers.Conv1D(layer_sizes[i],kernel_size=7, padding=\"same\",\n",
    "                                       bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
    "                                       activation=tf.nn.relu, name='conv%i' % (i+1)))\n",
    "            layers_list.append(\n",
    "                tf.keras.layers.Dropout(self.dropout_rate))\n",
    "            \n",
    "            layers_list.append(\n",
    "                tf.keras.layers.MaxPool1D(pool_size=5,strides=1,padding=\"same\"))\n",
    "        \n",
    "        \n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Flatten())\n",
    "        \n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dense(20, activation=tf.nn.relu, name='fc1'))\n",
    "        \n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        \n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dense(800, name='fc2'))\n",
    "       \n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Activation(tf.nn.softmax))\n",
    "        return layers_list\n",
    "\n",
    "\n",
    "    def call(self, x, training=False, get_logits=False):\n",
    "        \n",
    "        for layer_index, layer in enumerate(self.layers_list):\n",
    "            \n",
    "            if layer_index == (len(self.layers_list)-1) and get_logits==True:\n",
    "                return x \n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x_data, y_labels):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.call(x_data, training=True, get_logits=True)\n",
    "            loss, accuracy = loss_fn(logits, y_labels)\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def eval_step(self, x_data, y_labels):\n",
    "        \n",
    "        logits = self.call(x_data, training=False, get_logits=True)\n",
    "        loss, accuracy = loss_fn(logits, y_labels)\n",
    "        return loss, accuracy\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def prediction_step(self, x_data):\n",
    "        predictions = self.call(x_data, training=False, get_logits=False)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def write_to_train_summary(self, train_summary_writer, step, loss, accuracy):\n",
    "        with train_summary_writer.as_default():\n",
    "            for layer_i in self.layers_list:\n",
    "                layer_name = layer_i.name\n",
    "                if 'dense' in layer_name:\n",
    "                    weights, biases = layer_i.get_weights()\n",
    "                    tf.summary.histogram(layer_name + '_weights', weights, step=step)\n",
    "                    tf.summary.histogram(layer_name + '_biases', biases, step=step)\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "\n",
    "            \n",
    "    def write_to_val_summary(self, val_summary_writer, step, loss, accuracy):\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "    \n",
    "    \n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        # Creacion de 'writers' que guardan datos para Tensorboard\n",
    "        train_summary_writer = tf.summary.create_file_writer(\n",
    "            self.logdir + '/train')\n",
    "        val_summary_writer = tf.summary.create_file_writer(self.logdir + '/val')\n",
    "        print('\\n\\n[Beginning training of MLP at logdir \"%s\"]\\n' % (self.logdir,))    \n",
    "        # Definicion de variables utiles para el entrenamiento\n",
    "        n_batches = int(X_train.shape[0] / self.batch_size)\n",
    "        prev_validation_loss = 100.0\n",
    "        validation_period = 100\n",
    "        early_stop_flag = False\n",
    "        start_time = time.time()\n",
    "        iteration_history = []\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        # Se crea dataset para iterar sobre los batch de los datos de entreneminto. \n",
    "        # Para cada nueva epoca, se hacer un shuffle al set de train\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_train, y_train)).shuffle(X_train.shape[0]).batch(self.batch_size)\n",
    "        # Ciclo que recorre una epoca completa de los datos cada vez\n",
    "        for epoch in range(self.max_epochs):\n",
    "            if early_stop_flag:\n",
    "                # Si early stopping se activo, detener el entrenamiento\n",
    "                break            \n",
    "            # Ciclo que recorre los mini batches del set de train\n",
    "            for i, (X_batch, y_batch) in enumerate(train_dataset):\n",
    "                if early_stop_flag:\n",
    "                    # Si early stopping se activo, detener el entrenamiento\n",
    "                    break  \n",
    "                iteration = epoch * n_batches + i                    \n",
    "                # Ejecutar una iteracion de gradiente\n",
    "                self.train_step(X_batch, y_batch)\n",
    "                # Obtener estadisticas del entrenamiento\n",
    "                if iteration % validation_period == 0:\n",
    "                    iteration_history.append(iteration)\n",
    "                    # Estadisticas en el set de validacion\n",
    "                    val_loss, val_acc = self.eval_step(X_val, y_val)\n",
    "                    # Escribir estadisticas de validacion en tensorboard\n",
    "                    self.write_to_val_summary(\n",
    "                        val_summary_writer, iteration, val_loss, val_acc)\n",
    "                    val_loss_history.append(val_loss)\n",
    "                    val_acc_history.append(val_acc)\n",
    "                    # Estadisticas en el set de entrenamiento\n",
    "                    train_loss, train_acc = self.eval_step(X_train, y_train)\n",
    "                    # Escribir estadisticas e histogramas de parametros de \n",
    "                    # entrenamiento en tensorboard\n",
    "                    self.write_to_train_summary(\n",
    "                        train_summary_writer, iteration, train_loss, train_acc)\n",
    "                    train_loss_history.append(train_loss)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    print('Epoch: %d/%d, iter: %d. ' %\n",
    "                          (epoch+1, self.max_epochs, iteration), end='')\n",
    "                    print('Loss (train/val): %.3f / %.3f. Val. acc: %.1f%%' %\n",
    "                          (train_loss, val_loss, val_acc * 100), end='')\n",
    "                    \n",
    "                    # Chequear condicion de early_stopping\n",
    "                    if self.early_stopping is not None:\n",
    "                        if val_loss > prev_validation_loss:\n",
    "                            validation_checks += 1\n",
    "                        else:\n",
    "                            validation_checks = 0\n",
    "                            prev_validation_loss = val_loss\n",
    "                        print(', Val. checks: %d/%d' %\n",
    "                              (validation_checks, self.early_stopping))\n",
    "                        if validation_checks >= self.early_stopping:\n",
    "                            early_stop_flag = True\n",
    "                            print('Early stopping')\n",
    "                    else:\n",
    "                        print('')\n",
    "            elap_time = time.time()-start_time\n",
    "            print(\"Epoch finished. Elapsed time %1.4f [s]\\n\" % (elap_time,))\n",
    "        # Guardar estadisticas en un diccionario\n",
    "        train_stats = {\n",
    "            'iteration_history': np.array(iteration_history),\n",
    "            'train_loss_history': np.array(train_loss_history),\n",
    "            'train_acc_history': np.array(train_acc_history),\n",
    "            'val_loss_history': np.array(val_loss_history),\n",
    "            'val_acc_history': np.array(val_acc_history)\n",
    "        }\n",
    "        # Guardar grafo de evaluacion en tensorboard\n",
    "        self.write_graph_to_summary(train_summary_writer, X_train)\n",
    "        return train_stats\n",
    "\n",
    "    def write_graph_to_summary(self, train_summary_writer, X_train):\n",
    "       \n",
    "        logdir = self.logdir + '/graph'\n",
    "        tf.summary.trace_on(graph=True, profiler=False)\n",
    "        predicted_proba = self.prediction_step(X_train)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=\"prediction_step\",\n",
    "                step=0,\n",
    "                profiler_outdir=logdir)\n",
    "            tf.summary.trace_off()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Obtener las probabilidades de salida de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        return predicted_proba.numpy()\n",
    "    \n",
    "    def predict_label(self, X):\n",
    "       \n",
    "        # Obtener la probabilidad de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        # Etiquetar segun la etiqueta mas probable\n",
    "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"experiment_1\"\n",
    "\n",
    "# --- NO TOCAR\n",
    "logdir_father = \"./proyecto_logs/\"\n",
    "logdir = logdir_father + experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./proyecto_logs/experiment_1/run_0\"]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-58-1c3545a10c4e>:80 train_step  *\n        loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n    <ipython-input-64-f949a7454c6f>:25 loss_fn  *\n        loss = tf.reduce_mean(\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:244 sigmoid_cross_entropy_with_logits_v2\n        logits=logits, labels=labels, name=name)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((20, 800) vs (20, 800, 1, 800))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-eef63b41c087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# ----- Entrenamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrain_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstats_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-1c3545a10c4e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[1;31m# Ejecutar una iteracion de gradiente\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m                 \u001b[1;31m# Obtener estadisticas del entrenamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidation_period\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     \u001b[1;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3734\u001b[0m     \u001b[1;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-58-1c3545a10c4e>:80 train_step  *\n        loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n    <ipython-input-64-f949a7454c6f>:25 loss_fn  *\n        loss = tf.reduce_mean(\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:244 sigmoid_cross_entropy_with_logits_v2\n        logits=logits, labels=labels, name=name)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\joaqu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((20, 800) vs (20, 800, 1, 800))\n"
     ]
    }
   ],
   "source": [
    "run_n_times = 1\n",
    "stats_history = []\n",
    "for run in range(run_n_times):\n",
    "    # ----- Creacion de MLP\n",
    "    model1 = ModelClassifier(\n",
    "        layer_sizes=[40, 40, 40, 40],\n",
    "        input_shape=(800),\n",
    "        loss_function_name ='categorical_crossentropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=20,\n",
    "        max_epochs=10,\n",
    "        dropout_rate=0.5,\n",
    "        early_stopping=15,\n",
    "        logdir=logdir+'/run_%d' % run)\n",
    "\n",
    "    # ----- Entrenamiento \n",
    "    \n",
    "    train_stats = model1.fit(x_train2, y_train2, x_val2, y_val2)\n",
    "    model1.summary()\n",
    "    stats_history.append(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4463, 800, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
